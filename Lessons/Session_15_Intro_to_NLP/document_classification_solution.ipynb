{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z0IdL5VTHiAG"
   },
   "outputs": [],
   "source": [
    "# Text Classification of Amazon, Yelp & IMDB Reviews using TF IDF Matrix\n",
    "\n",
    "# Importing the libraries\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# Importing dataset\n",
    "yelp_ds = pd.read_csv('yelp_labelled.txt', sep ='\\t', header = None, names = ['reviews', 'rating'])\n",
    "amazon_ds = pd.read_csv('amazon_cells_labelled.txt', sep ='\\t', header = None, names = ['reviews', 'rating'])\n",
    "imdb_ds = pd.read_csv('imdb_labelled.txt', sep ='\\t', header = None, names = ['reviews', 'rating'])\n",
    "\n",
    "#combine the datasets\n",
    "dataset = pd.concat([yelp_ds, amazon_ds, imdb_ds], ignore_index = True)\n",
    "\n",
    "# Manually setting the rating for 2 reviews with NaN values\n",
    "dataset.fillna(1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow loved place</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crust good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tasty texture nasty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selection menu great price</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating\n",
       "0                                    wow loved place       1\n",
       "1                                         crust good       0\n",
       "2                                tasty texture nasty       0\n",
       "3  stopped late may bank holiday rick steve recom...       1\n",
       "4                         selection menu great price       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Wow... Loved this place.\n",
       "1                                   Crust is not good.\n",
       "2            Not tasty and the texture was just nasty.\n",
       "3    Stopped by during the late May bank holiday of...\n",
       "4    The selection on the menu was great and so wer...\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['reviews'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "K9UON10_HTIP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/youssefmecky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/youssefmecky/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/youssefmecky/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Defining a Function to clean up the reviews\n",
    "def text_preprocess(ds: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply NLP Preprocessing Techniques to the reviews.\n",
    "\n",
    "    Parameters:\n",
    "    ds (pd.Series): A pandas Series containing the text data to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A pandas Series with cleaned and preprocessed text.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(text):\n",
    "        # Remove non-alphanumeric characters and convert to lowercase\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "        # Tokenize words\n",
    "        words = text.split()\n",
    "        # Remove stopwords and lemmatize each word\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "        # Join back to form the processed sentence\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    # Apply the cleaning function to the entire Series\n",
    "    return ds.apply(clean_text)\n",
    "\n",
    "# Assuming dataset is a DataFrame with a 'reviews' column\n",
    "dataset['reviews'] = text_preprocess(dataset['reviews'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      wow loved place\n",
       "1                                           crust good\n",
       "2                                  tasty texture nasty\n",
       "3    stopped late may bank holiday rick steve recom...\n",
       "4                           selection menu great price\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['reviews'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HBQ_ie-UIJdS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.7709090909090909\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77       285\n",
      "           1       0.75      0.79      0.77       265\n",
      "\n",
      "    accuracy                           0.77       550\n",
      "   macro avg       0.77      0.77      0.77       550\n",
      "weighted avg       0.77      0.77      0.77       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building a TF-IDF matrix out of the corpus of reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# 'reviews' column should already be preprocessed\n",
    "\n",
    "# Step 1: Vectorize the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "X = tfidf_vectorizer.fit_transform(dataset['reviews'])\n",
    "y = dataset['rating']  # Target labels\n",
    "\n",
    "# Step 2: Splitting into training & test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Training the classifier & predicting on test data\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Step 4: Classification metrics\n",
    "print('\\nAccuracy:', accuracy_score(y_test, y_pred))\n",
    "print('\\nClassification Report')\n",
    "print('======================================================')\n",
    "print('\\n', classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
